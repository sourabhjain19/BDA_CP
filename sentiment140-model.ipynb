{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spark\n",
      "  Downloading spark-0.2.1.tar.gz (41 kB)\n",
      "\u001b[K     |████████████████████████████████| 41 kB 30 kB/s  eta 0:00:011\n",
      "\u001b[?25hBuilding wheels for collected packages: spark\n",
      "  Building wheel for spark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for spark: filename=spark-0.2.1-py3-none-any.whl size=58738 sha256=343d718b20945e68e4f3b65d333db179c2f080fe9c74c058565c37d51c153e76\n",
      "  Stored in directory: /root/.cache/pip/wheels/4e/0e/f1/164619f9920fb447d294afaae11a7715bd442ded7225953d72\n",
      "Successfully built spark\n",
      "Installing collected packages: spark\n",
      "Successfully installed spark-0.2.1\n",
      "Collecting pyspark\n",
      "  Downloading pyspark-3.0.1.tar.gz (204.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 204.2 MB 27 kB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting py4j==0.10.9\n",
      "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
      "\u001b[K     |████████████████████████████████| 198 kB 46.3 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612244 sha256=be236b9daedf4a7b0a51527ac675462bd7eaca54163bf691d7a2e95e41ffce02\n",
      "  Stored in directory: /root/.cache/pip/wheels/5e/34/fa/b37b5cef503fc5148b478b2495043ba61b079120b7ff379f9b\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9 pyspark-3.0.1\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement warnings (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for warnings\u001b[0m\n",
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.8.tar.gz (981 kB)\n",
      "\u001b[K     |████████████████████████████████| 981 kB 4.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from langdetect) (1.14.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.8-py3-none-any.whl size=993191 sha256=03b018b9fbbce29fcba7d54b365673bd0d9b9a722080f4571c8f484ab4afc94f\n",
      "  Stored in directory: /root/.cache/pip/wheels/59/f6/9d/85068904dba861c0b9af74e286265a08da438748ee5ae56067\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.8\n"
     ]
    }
   ],
   "source": [
    "!pip install spark\n",
    "!pip install pyspark\n",
    "!pip install warnings\n",
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0e0a5c51bbff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtextblob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangdetect\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdetect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "import pyspark as ps\n",
    "import warnings\n",
    "from pyspark.sql import SQLContext\n",
    "from textblob import TextBlob\n",
    "from langdetect import detect\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just created a SparkContext\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    sc = ps.SparkContext('local[10]')\n",
    "    sqlContext = SQLContext(sc)\n",
    "    print(\"Just created a SparkContext\")\n",
    "except ValueError:\n",
    "    warnings.warn(\"SparkContext already exists in this scope\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = sqlContext.read.format('com.databricks.spark.csv').options(header='false', inferschema='false').load('../input/sentiment140/training.1600000.processed.noemoticon.csv')\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "|_c0|       _c1|                 _c2|     _c3|            _c4|                 _c5|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "|  0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|  0|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|  0|1467810917|Mon Apr 06 22:19:...|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "|  0|1467811184|Mon Apr 06 22:19:...|NO_QUERY|        ElleCTF|my whole body fee...|\n",
      "|  0|1467811193|Mon Apr 06 22:19:...|NO_QUERY|         Karoli|@nationwideclass ...|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|                 _c5|_c0|\n",
      "+--------------------+---+\n",
      "|@switchfoot http:...|  0|\n",
      "|is upset that he ...|  0|\n",
      "|@Kenichan I dived...|  0|\n",
      "|my whole body fee...|  0|\n",
      "|@nationwideclass ...|  0|\n",
      "|@Kwesidei not the...|  0|\n",
      "|          Need a hug|  0|\n",
      "|@LOLTrish hey  lo...|  0|\n",
      "|@Tatiana_K nope t...|  0|\n",
      "|@twittera que me ...|  0|\n",
      "|spring break in p...|  0|\n",
      "|I just re-pierced...|  0|\n",
      "|@caregiving I cou...|  0|\n",
      "|@octolinz16 It it...|  0|\n",
      "|@smarrison i woul...|  0|\n",
      "|@iamjazzyfizzle I...|  0|\n",
      "|Hollis' death sce...|  0|\n",
      "| about to file taxes|  0|\n",
      "|@LettyA ahh ive a...|  0|\n",
      "|@FakerPattyPattz ...|  0|\n",
      "+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df = df.select('_c5','_c0')\n",
    "df = df.na.drop()\n",
    "df = df.withColumn('_c5', trim(col('_c5')))\n",
    "df = df.filter(col('_c5')!=\"\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf, col\n",
    "\n",
    "@udf(returnType=IntegerType())\n",
    "def langdetect(tweet):\n",
    "    try:\n",
    "        if detect(tweet)=='en':\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except:\n",
    "        return 0\n",
    "@udf(returnType=StringType())\n",
    "def clean_tweet(tweet): \n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "    \n",
    "@udf(returnType=FloatType())\n",
    "def sentiment_score(tweet):\n",
    "    return TextBlob(tweet).sentiment.polarity\n",
    "\n",
    "@udf(returnType=IntegerType())\n",
    "def get_sentiment(score): \n",
    "        if score > 0: \n",
    "            return 4\n",
    "        elif score == 0: \n",
    "            return 2\n",
    "        else: \n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+------------+--------------------+-----------+------+\n",
      "|                 _c5|_c0|englishornot|          cleantweet|   polarity|output|\n",
      "+--------------------+---+------------+--------------------+-----------+------+\n",
      "|@switchfoot http:...|  0|           1|Awww that s a bum...|        0.2|     4|\n",
      "|is upset that he ...|  0|           1|is upset that he ...|        0.0|     2|\n",
      "|@Kenichan I dived...|  0|           1|I dived many time...|        0.5|     4|\n",
      "|my whole body fee...|  0|           1|my whole body fee...|        0.2|     4|\n",
      "|@nationwideclass ...|  0|           1|no it s not behav...|     -0.625|     0|\n",
      "|@Kwesidei not the...|  0|           1|  not the whole crew|        0.2|     4|\n",
      "|          Need a hug|  0|           1|          Need a hug|        0.0|     2|\n",
      "|@LOLTrish hey  lo...|  0|           1|hey long time no ...| 0.27333334|     4|\n",
      "|@Tatiana_K nope t...|  0|           1|K nope they didn ...|        0.0|     2|\n",
      "|spring break in p...|  0|           1|spring break in p...|-0.21428572|     0|\n",
      "|I just re-pierced...|  0|           1|I just re pierced...|        0.0|     2|\n",
      "|@caregiving I cou...|  0|           1|I couldn t bear t...|        0.0|     2|\n",
      "|@octolinz16 It it...|  0|           1|It it counts idk ...|        0.0|     2|\n",
      "|@smarrison i woul...|  0|           1|i would ve been t...|      0.075|     4|\n",
      "|@iamjazzyfizzle I...|  0|           1|I wish I got to w...|        0.0|     2|\n",
      "|Hollis' death sce...|  0|           1|Hollis death scen...|        0.0|     2|\n",
      "| about to file taxes|  0|           1| about to file taxes|        0.0|     2|\n",
      "|@LettyA ahh ive a...|  0|           1|ahh ive always wa...|        0.5|     4|\n",
      "|@FakerPattyPattz ...|  0|           1|Oh dear Were you ...|        0.0|     2|\n",
      "|@alydesigns i was...|  0|           1|i was out most of...|       0.35|     4|\n",
      "+--------------------+---+------------+--------------------+-----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=df.withColumn(\"englishornot\", langdetect(col(\"_c5\")))\n",
    "df = df.filter(col('englishornot')!=0)\n",
    "df=df.withColumn(\"cleantweet\", clean_tweet(col(\"_c5\")))\n",
    "df=df.withColumn(\"polarity\", sentiment_score(col(\"cleantweet\")))\n",
    "df=df.withColumn(\"output\", get_sentiment(col(\"polarity\")))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|          cleantweet|_c0|\n",
      "+--------------------+---+\n",
      "|Awww that s a bum...|  0|\n",
      "|is upset that he ...|  0|\n",
      "|I dived many time...|  0|\n",
      "|my whole body fee...|  0|\n",
      "|no it s not behav...|  0|\n",
      "|  not the whole crew|  0|\n",
      "|          Need a hug|  0|\n",
      "|hey long time no ...|  0|\n",
      "|K nope they didn ...|  0|\n",
      "|spring break in p...|  0|\n",
      "|I just re pierced...|  0|\n",
      "|I couldn t bear t...|  0|\n",
      "|It it counts idk ...|  0|\n",
      "|i would ve been t...|  0|\n",
      "|I wish I got to w...|  0|\n",
      "|Hollis death scen...|  0|\n",
      "| about to file taxes|  0|\n",
      "|ahh ive always wa...|  0|\n",
      "|Oh dear Were you ...|  0|\n",
      "|i was out most of...|  0|\n",
      "+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.select('cleantweet','_c0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_set, val_set, test_set) = df.randomSplit([0.98, 0.01, 0.01], seed = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+-----+--------------------+--------------------+-----+\n",
      "|cleantweet|_c0|words|                  tf|            features|label|\n",
      "+----------+---+-----+--------------------+--------------------+-----+\n",
      "|          |  0|   []|(65536,[52572],[1...|(65536,[52572],[7...|  0.0|\n",
      "|          |  0|   []|(65536,[52572],[1...|(65536,[52572],[7...|  0.0|\n",
      "|          |  0|   []|(65536,[52572],[1...|(65536,[52572],[7...|  0.0|\n",
      "|          |  0|   []|(65536,[52572],[1...|(65536,[52572],[7...|  0.0|\n",
      "|          |  0|   []|(65536,[52572],[1...|(65536,[52572],[7...|  0.0|\n",
      "+----------+---+-----+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"cleantweet\", outputCol=\"words\")\n",
    "hashtf = HashingTF(numFeatures=2**16, inputCol=\"words\", outputCol='tf')\n",
    "idf = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "label_stringIdx = StringIndexer(inputCol = \"_c0\", outputCol = \"label\")\n",
    "pipeline = Pipeline(stages=[tokenizer, hashtf, idf, label_stringIdx])\n",
    "\n",
    "pipelineFit = pipeline.fit(train_set)\n",
    "train_df = pipelineFit.transform(train_set)\n",
    "val_df = pipelineFit.transform(val_set)\n",
    "train_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1ce2cb6a2da8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxIter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlrModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlrModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(maxIter=100)\n",
    "lrModel = lr.fit(train_df)\n",
    "predictions = lrModel.transform(val_df)\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(val_set.count())\n",
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
